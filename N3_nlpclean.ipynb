{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK and cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/train/00001756c60be8.json\",'r') as f: \n",
    "   jsondict = json.load(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/train/0a06dfd3d34f34.json\",'r') as f: \n",
    "   jsondict2 = json.load(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# This Python 3 environment comes with many helpful analytics libraries installed\n",
      "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
      "# For example, here's several helpful packages to load\n",
      "\n",
      "import numpy as np # linear algebra\n",
      "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
      "\n",
      "# Input data files are available in the read-only \"../input/\" directory\n",
      "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
      "\n",
      "import os\n",
      "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
      "    for filename in filenames:\n",
      "        print(os.path.join(dirname, filename))\n",
      "\n",
      "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
      "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n"
     ]
    }
   ],
   "source": [
    "print(jsondict[\"source\"]['1862f0a6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['1862f0a6', '2a9e43d6', '038b763d', '2eefe0ef', '0beab1cd', '9a78ab76', 'ebe125d5', 'd9dced8b', '86497fe1', 'e2c8e725', 'ff7c44ed', '0e7c906e', 'dd0c804a', '781bbf3c', 'bd94f005', '62638fba', 'bb69e88c', '6b5664c7', '23783525', '8522781a', '8ca8392c', '17ec3fc4', '76512d50', 'a98c5d9f', '06365725', '59959af5', '80151ab7', '5bf9ca51', 'f5504853', '9f50dca0', '21616367', 'fcb6792d', '63c26fa2', '4bb2e30a', 'a6357f7e', '45082c89', '77e56113', '448eb224', '032e2820', '8554b284', '36002912', 'ac301a84', '23705731', '1496beaf', '2e1a5949', '7e2f170a', 'bfbde93e', '0d136e08', '915643b3', '8ffe0b25', '8a4c95d1', 'b69a4f9b', 'c3ce0945', '3eebeb87', '1ae087ab', 'aaad8355', '503926eb', '3e5f860d'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsondict[\"source\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['f18fe90f', '7b146807', '7fc42e3d', 'e4187045', 'f3f1f1ca', '64963778', '64fe4e2b', '707dcebc', '98f64a54', 'b5d0e650', 'a332db8d', '214b3983', 'b24b25fc', '5b3482ba', '948395b6', '61b20a7e', '45258e3a', '4f0c6a57', '1ac84d33', '53238e89', '632ba6b6', '81238c6e', 'c8f8caee', '5ed60d72', 'a44a736b', 'e0fa466d', 'fb7c7020', 'a5b26c3f', '280ac2bc', '826a3eb5', '6fa0283e', '8ca1a048', '8ca9d124', '69220509', 'fd1cac48', '57db3379', 'fb23cc8a', '43c994f4', '31203890', '0b730456', '0950a6f1', 'e2ebae3a', '3f658999', '208b4af1', 'dc293fd5', '3ab72d73', 'dc4c3dda', 'a48b5ead', 'f942541a', '33bc1f76', '69556613', '22ca3e04', '41b07cec', 'aeadecf5', 'bded11a2', '9673ab13', '3b249172', 'b95dfce2', 'e9cc8f46', 'ac401605', '37f975f7'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsondict2[\"source\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'*Тип данных обучающего сета*'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testtext1 = jsondict[\"source\"]['aaad8355']\n",
    "testtext1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## The Goal\\n\\nThe Source for this competition is: \\n\\nhttps://www.kaggle.com/c/learn-together/data\\n\\nThe study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. Each observation is a 30m x 30m patch. Our goal is to predict an integer classification for the forest cover type. The seven types are:\\n\\n1 - Spruce/Fir\\n\\n2 - Lodgepole Pine\\n\\n3 - Ponderosa Pine\\n\\n4 - Cottonwood/Willow\\n\\n5 - Aspen\\n\\n6 - Douglas-fir\\n\\n7 - Krummholz\\n\\nThe training set (15120 observations) contains both features and the Cover_Type. The test set contains only the features. You must predict the Cover_Type for every row in the test set (565892 observations).'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testtext2 = jsondict2[\"source\"]['b95dfce2']\n",
    "testtext2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fig, ax =plt.subplots(4,3, figsize=(22,20))\\nsns.boxplot(\"Soil_Type1\", \"Elevation\", data=train, ax=ax[0][0])\\nsns.boxplot(\"Soil_Type2\", \"Aspect\", data=train, ax=ax[0][1])\\nsns.boxplot(\"Soil_Type3\", \"Slope\", data=train, ax=ax[0][2])\\nsns.boxplot(\"Soil_Type4\", \"Horizontal_Distance_To_Hydrology\", data=train, ax=ax[1][0])\\nsns.boxplot(\"Soil_Type5\", \"Vertical_Distance_To_Hydrology\", data=train, ax=ax[1][1])\\nsns.boxplot(\"Soil_Type6\", \"Horizontal_Distance_To_Roadways\", data=train, ax=ax[1][2])\\nsns.boxplot(\"Soil_Type7\", \"Horizontal_Distance_To_Fire_Points\", data=train, ax=ax[2][0])\\nsns.boxplot(\"Soil_Type8\", \"Hillshade_9am\", data=train, ax=ax[2][1])\\nsns.boxplot(\"Soil_Type9\", \"Hillshade_Noon\", data=train, ax=ax[2][2])\\nsns.boxplot(\"Soil_Type10\", \"Hillshade_3pm\", data=train, ax=ax[3][0])\\nsns.boxplot(\"Soil_Type11\", \"Hillshade_9am\", data=train, ax=ax[3][1])\\nsns.boxplot(\"Soil_Type12\", \"Hillshade_Noon\", data=train, ax=ax[3][2])\\n\\nfig.show()'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testtext3 = jsondict2[\"source\"]['31203890']\n",
    "testtext3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try out NLTK\n",
    "\n",
    "tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kakw/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   the goal  the source for this competition is    https   www kaggle com c learn together data  the study area includes four wilderness areas located in the roosevelt national forest of northern colorado  each observation is a 30m x 30m patch  our goal is to predict an integer classification for the forest cover type  the seven types are   1   spruce fir  2   lodgepole pine  3   ponderosa pine  4   cottonwood willow  5   aspen  6   douglas fir  7   krummholz  the training set  15120 observations  contains both features and the cover type  the test set contains only the features  you must predict the cover type for every row in the test set  565892 observations  '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text = re.sub(r\"[^a-zA-Z0-9]\", \" \", testtext2.lower())\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence tokenize\n",
    "This is perhaps not super useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['## The Goal\\n\\nThe Source for this competition is: \\n\\nhttps://www.kaggle.com/c/learn-together/data\\n\\nThe study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado.',\n",
       " 'Each observation is a 30m x 30m patch.',\n",
       " 'Our goal is to predict an integer classification for the forest cover type.',\n",
       " 'The seven types are:\\n\\n1 - Spruce/Fir\\n\\n2 - Lodgepole Pine\\n\\n3 - Ponderosa Pine\\n\\n4 - Cottonwood/Willow\\n\\n5 - Aspen\\n\\n6 - Douglas-fir\\n\\n7 - Krummholz\\n\\nThe training set (15120 observations) contains both features and the Cover_Type.',\n",
       " 'The test set contains only the features.',\n",
       " 'You must predict the Cover_Type for every row in the test set (565892 observations).']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(testtext2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'goal', 'the', 'source', 'for', 'this', 'competition', 'is', 'https', 'www']\n"
     ]
    }
   ],
   "source": [
    "words=nltk.word_tokenize(text)\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/kakw/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['goal', 'source', 'competition', 'https', 'www', 'kaggle', 'com', 'c', 'learn', 'together', 'data', 'study', 'area', 'includes', 'four', 'wilderness', 'areas', 'located', 'roosevelt', 'national']\n"
     ]
    }
   ],
   "source": [
    "words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "print(words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['goal', 'sourc', 'competit', 'http', 'www', 'kaggl', 'com', 'c', 'learn', 'togeth', 'data', 'studi', 'area', 'includ', 'four', 'wilder', 'area', 'locat', 'roosevelt', 'nation', 'forest', 'northern', 'colorado', 'observ', '30m', 'x', '30m', 'patch', 'goal', 'predict', 'integ', 'classif', 'forest', 'cover', 'type', 'seven', 'type', '1', 'spruce', 'fir', '2', 'lodgepol', 'pine', '3', 'ponderosa', 'pine', '4', 'cottonwood', 'willow', '5', 'aspen', '6', 'dougla', 'fir', '7', 'krummholz', 'train', 'set', '15120', 'observ', 'contain', 'featur', 'cover', 'type', 'test', 'set', 'contain', 'featur', 'must', 'predict', 'cover', 'type', 'everi', 'row', 'test', 'set', '565892', 'observ']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "# Reduce words to their stems\n",
    "stemmed = [PorterStemmer().stem(w) for w in words]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Cells\n",
    "\n",
    "Let's try the same thing on code cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_unit(testtext):\n",
    "    words = re.sub(r\"[^a-zA-Z0-9]\", \" \", testtext.lower())\n",
    "    words=nltk.word_tokenize(words)\n",
    "    words=[w for w in words if w not in stopwords.words(\"english\")]\n",
    "    #print([w for w in words if w not in stopwords.words(\"english\")])\n",
    "    words = [words for words in words if not words.isdigit()]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['goal', 'source', 'competition', 'https', 'www', 'kaggle', 'com', 'c', 'learn', 'together', 'data', 'study', 'area', 'includes', 'four', 'wilderness', 'areas', 'located', 'roosevelt', 'national', 'forest', 'northern', 'colorado', 'observation', '30m', 'x', '30m', 'patch', 'goal', 'predict', 'integer', 'classification', 'forest', 'cover', 'type', 'seven', 'types', 'spruce', 'fir', 'lodgepole', 'pine', 'ponderosa', 'pine', 'cottonwood', 'willow', 'aspen', 'douglas', 'fir', 'krummholz', 'training', 'set', 'observations', 'contains', 'features', 'cover', 'type', 'test', 'set', 'contains', 'features', 'must', 'predict', 'cover', 'type', 'every', 'row', 'test', 'set', 'observations']\n"
     ]
    }
   ],
   "source": [
    "words2 = preprocessing_unit(testtext2); print(words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fig', 'ax', 'plt', 'subplots', 'figsize', 'sns', 'boxplot', 'soil', 'type1', 'elevation', 'data', 'train', 'ax', 'ax', 'sns', 'boxplot', 'soil', 'type2', 'aspect', 'data', 'train', 'ax', 'ax', 'sns', 'boxplot', 'soil', 'type3', 'slope', 'data', 'train', 'ax', 'ax', 'sns', 'boxplot', 'soil', 'type4', 'horizontal', 'distance', 'hydrology', 'data', 'train', 'ax', 'ax', 'sns', 'boxplot', 'soil', 'type5', 'vertical', 'distance', 'hydrology', 'data', 'train', 'ax', 'ax', 'sns', 'boxplot', 'soil', 'type6', 'horizontal', 'distance', 'roadways', 'data', 'train', 'ax', 'ax', 'sns', 'boxplot', 'soil', 'type7', 'horizontal', 'distance', 'fire', 'points', 'data', 'train', 'ax', 'ax', 'sns', 'boxplot', 'soil', 'type8', 'hillshade', '9am', 'data', 'train', 'ax', 'ax', 'sns', 'boxplot', 'soil', 'type9', 'hillshade', 'noon', 'data', 'train', 'ax', 'ax', 'sns', 'boxplot', 'soil', 'type10', 'hillshade', '3pm', 'data', 'train', 'ax', 'ax', 'sns', 'boxplot', 'soil', 'type11', 'hillshade', '9am', 'data', 'train', 'ax', 'ax', 'sns', 'boxplot', 'soil', 'type12', 'hillshade', 'noon', 'data', 'train', 'ax', 'ax', 'fig', 'show']\n"
     ]
    }
   ],
   "source": [
    "words3 = preprocessing_unit(testtext3); print(words3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams, FreqDist\n",
    "all_counts3 = dict()\n",
    "for size in 1, 2, 3, 4, 5:\n",
    "    all_counts3[size] = FreqDist(ngrams(words3, size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams, FreqDist\n",
    "all_counts2 = dict()\n",
    "for size in 1, 2, 3, 4, 5:\n",
    "    all_counts2[size] = FreqDist(ngrams(words2, size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cover 3\n",
      "type 3\n",
      "set 3\n",
      "goal 2\n",
      "forest 2\n"
     ]
    }
   ],
   "source": [
    "for ng, ngcount in all_counts[1].most_common(5):\n",
    "    print(\" \".join(ng), ngcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cover type 3\n",
      "set observations 2\n",
      "contains features 2\n",
      "test set 2\n",
      "goal source 1\n"
     ]
    }
   ],
   "source": [
    "for ng, ngcount in all_counts[2].most_common(5):\n",
    "    print(\" \".join(ng), ngcount)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4976e0179d97dd6d59b1329a76e601e17b789c2571b41c8b57f5fd69821c0dd3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
